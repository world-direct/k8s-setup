## Global Setup options
#############################################################

# The global_vagrant_* are ignored, when the global_mode is not 'vagranu'
global_vagrant_lnxclp_count: 1
global_vagrant_lnxclp_mem: 2048
global_vagrant_lnxclp_cpu: 2

global_vagrant_lnxwrk_count: 1
global_vagrant_lnxwrk_mem: 2048
global_vagrant_lnxwrk_cpu: 1

global_vagrant_winwrk_count: 0
global_vagrant_winwrk_mem: 2048
global_vagrant_winwrk_cpu: 2

global_vagrant_lnx_boxname: centos/7
global_vagrant_win_boxname: StefanScherer/windows_2019

global_vagrant_hosts_network: 10.0.0.*

# This option is only needed for production configuration.
# The 'vagrant' mode generates an own inventory
# A relative path is interpreted relative to the directory of the configuration file
ansible_inventory_filepath: 

## Version Options
#############################################################

# this is the target kubernetes version
# Provisioning the cluster will bring the cluster to this version
# Currently only install is implemented, but after implementing upgrade,
# you can control upgrading as well with this option.
k8s_version: 1.16.3

# recommended version
# https://kubernetes.io/docs/setup/production-environment/container-runtimes/#docker
# see versions: https://download.docker.com/linux/centos/7/x86_64/stable/Packages/
# https://docs.docker.com/engine/release-notes/
lnx_docker_version: "18.06.3"
win_docker_version: "19.03.3"


## Setup options
#############################################################

# if this is enabled, all yum packages are updated on host provisioning
setup_update_all_packages: no

# the interface to use
host_primary_interface_name: eth0

## Kubernetes global cluster configuration
#############################################################

# The port of the load-balanced API Server
k8s_apiserver_port: 443

# the CIDR used for the pod network, need to be used for routing
k8s_pod_network_cidr: 10.244.0.0/16

# the Network used internally by kubernetes for ClusterIP services
k8s_service_cidr: 10.96.0.0/16

# this is the VIP of the clp load balancer
k8s_apiserver_vip: 

# the base dnsname for the cluster  **)
k8s_cluster_dnsname: 

# the hostname used for the apiserver (without the k8s_cluster_dnsname) **)
k8s_apiserver_hostname: apiserver

# to install the flannel CNI plugin
# we will evalute the 'Host Gateway Mode' as an alternative, specially how it 
# plays well with kubadm
k8s_install_flannel: yes

# this is the IP address to use for the Ingress Controller
k8s_loadbalancers_ingress_ip:           # example: 10.0.0.80

# this is the IP range for all other services of type=LoadBalancer
k8s_loadbalancers_default_ip_range:     # example: 10.0.0.81-10.0.0.99

## CA Configuration
#############################################################

## 'CA' mode: This mode uses a file-based CA to issue Server-Certificates
## with 'certmanager' (see https://cert-manager.io/docs/configuration/ca/)
##
## Enable CA mode by:
# k8s_certs_mode: CA
#
## If you want k8s-setup to automatically generate a CA keypair for you:
# k8s_certs_ca:
#   generate: true
## If you want an existing CA (like an intermediate CA for the cluster) to be used
## This assumes that there is a sub-directory 'ca' within your configuration-directory,
## where the ctr.pem and key.pem files are contained.
# k8s_certs_ca:
#   crt_filepath: ca/crt.pem
#   key_filepath: ca/key.pem
#
## Notes: Regardless if the ca files are generated or existing files are used,
## the provisioner creates a secret named 'k8s-setup-certs-ca' in the 'kube-certmanager'
## namespace.
##
## Enable ACMD mode by:
# k8s_certs_mode: ACME
# k8s_certs_acme:
#   email: registereduser@mail.com  # the let's encrypt user
#   server: <url> # a well-known let's encrypt environment, or custom url
#   # 'staging' = https://acme-staging-v02.api.letsencrypt.org/directory (https://letsencrypt.org/docs/staging-environment/)
#   # 'production' = https://acme-v02.api.letsencrypt.org/directory



# the 'CA' mode uses an existing CA, or generates a CA dynamically
k8s_certs_mode: CA
k8s_certs_ca:
  generate: true    # generate a CA for the cluster on first provisioning


## Provisioning Client configuration
#############################################################

# In the cluster-local.yml playbook, the tools kubectl and helm3 will be fetched
# accounting to this versions:
# You may 'source /.local/scripts/use-context' script, to activate and validate
# The current kubectl context
client_helm_version: v3.0.0-rc.2
client_kubectl_version: v1.17.0

# This is the directory where kubectl and helm are stored
# This should be in $PATH for provisioning to workok
client_bin_dir: /usr/local/bin

## Trident information
#############################################################
trident:
  enabled: no
  version: 19.10.0

  # this is the configuration file that is used for tridentctl:
  # https://netapp-trident.readthedocs.io/en/stable-v19.10/kubernetes/operations/tasks/backends/index.html
  backend_config:

## Harbor information
#############################################################
harbor:
  enabled: no
  version: 1.9.3

## Custom firewall rules
#############################################################

lnxclp_custom_firewall_rules: []
lnxwrk_custom_firewall_rules: []
winwrk_custom_firewall_rules: []


## Dashboard options
#############################################################
dashboard_token_ttl_s: 10800 # Session-Timeout for dashboard (10800=3 Hours)

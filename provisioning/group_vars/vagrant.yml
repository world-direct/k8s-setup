# recommended version
# https://kubernetes.io/docs/setup/production-environment/container-runtimes/#docker
# see versions: https://download.docker.com/linux/centos/7/x86_64/stable/Packages/
# https://docs.docker.com/engine/release-notes/
lnx_docker_version: "18.06.3"
win_docker_version: "19.03.3"

## Setup options
#############################################################

# to reduce the time of provisioning, you may skip the system_setup phase,
# if this has already been done. This is just an optimization for development. 
setup_skip_system_setup: no

# if this is enabled, all yum packages are updated on start
setup_update_all_packages: no

# the playbook will only init a new cluster, if this enabled
# this is for safty, to make sure that an existing cluster is untouched
setup_create_new_k8s_cluster: yes

# the playbook will create k8sinit, k8sjoin and k8steardown scripts, but don't execute them
# this is specially for testing, to see timeing and stdout / stderr
setup_manual_kubeadm: false

## Kubernetes global cluster configuration
#############################################################

# The port of the load-balanced API Server
k8s_apiserver_port: 443

# the CIDR used for the pod network, need to be used for routing
k8s_pod_network_cidr: 10.244.0.0/16

# this is the VIP of the clp load balancer
k8s_api_server_vip: 10.0.0.2

# the base dnsname for the cluster  **)
k8s_cluster_dnsname: k8stest.local

# the hostname used for the apiserver (without the k8s_cluster_dnsname) **)
k8s_apiserver_hostname: apiserver

# to install the flannel CNI plugin
# we will evalute the 'Host Gateway Mode' as an alternative, specially how it 
# plays well with kubadm
k8s_install_flannel: yes

# this is the IP range available for LoadBalancer objects
# because the Ingress controller is created on setup, all ingresses use the first IP
k8s_metallb_ip_start: 10.0.0.80
k8s_metallb_ip_end: 10.0.0.99

## Provisioning Client configuration
#############################################################

# If 'yes', the kubeadm-first role, fetches the /etc/kubernetes/admin.conf
# from the first clp node to the client unter ~/.kube/config
# So this can be used by the default kubectl configuration
# kubectl and helm3 will be installed in ~/.local/bin/helm3

# If 'no', file is fetched to ~/{{k8s_cluster_dnsname}}/config
# kubectl and helm3 will be installed in ~/{{k8s_cluster_dnsname}}
# There will be a ~/{{k8s_cluster_dnsname}}/shell script, to register use this context
client_setup_default_context: no

client_helm_version: v3.0.0-rc.2
client_kubectl_version: v1.16.2

## Notes
#############################################################
#
# **): If you use Vagrant for local machine tests, these values will be
#       set from the .env file in the project root. Change them in this file
#
# Parameters, only passed by Vagrant:
#   k8s_enable_proxy: yes
#   host_primary_interface_name: eth1
